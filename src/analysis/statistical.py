# src/analysis/statistical.py (Fixed version)
import pandas as pd
import numpy as np
from scipy import stats
from typing import Dict, Tuple, List
import pingouin as pg
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from statsmodels.stats.power import ttest_power
import warnings
warnings.filterwarnings('ignore')

class EducationalStatisticsAnalyzer:
    """Comprehensive statistical analysis for educational data"""
    
    def __init__(self):
        self.results = {}
        
    def compare_learning_methods(self, data: pd.DataFrame, 
                               group_col: str, outcome_col: str) -> Dict:
        """Compare learning outcomes between different methods using t-test"""
        
        groups = data[group_col].unique()
        
        if len(groups) != 2:
            raise ValueError("T-test requires exactly 2 groups")
        
        group1_data = data[data[group_col] == groups[0]][outcome_col]
        group2_data = data[data[group_col] == groups[1]][outcome_col]
        
        # Check assumptions
        normality_results = {
            str(groups[0]): stats.shapiro(group1_data),
            str(groups[1]): stats.shapiro(group2_data)
        }
        
        # Levene's test for equal variances
        levene_stat, levene_p = stats.levene(group1_data, group2_data)
        
        # Choose appropriate test
        if all(p > 0.05 for _, p in normality_results.values()):
            # Parametric test
            if levene_p > 0.05:
                t_stat, p_value = stats.ttest_ind(group1_data, group2_data)
                test_type = "Independent t-test (equal variances)"
            else:
                t_stat, p_value = stats.ttest_ind(group1_data, group2_data, equal_var=False)
                test_type = "Welch's t-test (unequal variances)"
        else:
            # Non-parametric test
            t_stat, p_value = stats.mannwhitneyu(group1_data, group2_data, alternative='two-sided')
            test_type = "Mann-Whitney U test"
        
        # Effect size (Cohen's d)
        pooled_std = np.sqrt(
            ((len(group1_data) - 1) * group1_data.std()**2 + 
             (len(group2_data) - 1) * group2_data.std()**2) / 
            (len(group1_data) + len(group2_data) - 2)
        )
        cohens_d = (group1_data.mean() - group2_data.mean()) / pooled_std
        
        # Power analysis
        power = ttest_power(cohens_d, len(group1_data), alpha=0.05)
        
        results = {
            'test_type': test_type,
            'groups': {
                str(groups[0]): {
                    'mean': float(group1_data.mean()),
                    'std': float(group1_data.std()),
                    'n': int(len(group1_data)),
                    'normality_p': float(normality_results[str(groups[0])][1])
                },
                str(groups[1]): {
                    'mean': float(group2_data.mean()),
                    'std': float(group2_data.std()),
                    'n': int(len(group2_data)),
                    'normality_p': float(normality_results[str(groups[1])][1])
                }
            },
            'levene_test': {'statistic': float(levene_stat), 'p_value': float(levene_p)},
            'test_statistic': float(t_stat),
            'p_value': float(p_value),
            'effect_size': float(cohens_d),
            'statistical_power': float(power),
            'interpretation': self._interpret_effect_size(cohens_d),
            'significant': p_value < 0.05
        }
        
        return results
    
    def analyze_multi_factor_anova(self, data: pd.DataFrame, 
                                 dependent_var: str, 
                                 factors: List[str]) -> Dict:
        """Perform multi-factor ANOVA analysis"""
        
        # Create formula for ANOVA
        formula = f"{dependent_var} ~ " + " * ".join(factors)
        
        # Perform ANOVA using pingouin
        aov = pg.anova(data=data, dv=dependent_var, between=factors)
        
        # Assumptions testing
        # Normality test by group
        normality_results = {}
        for factor in factors:
            for group in data[factor].unique():
                group_data = data[data[factor] == group][dependent_var]
                stat, p_val = stats.shapiro(group_data)
                normality_results[f"{factor}_{group}"] = {
                    'statistic': float(stat), 
                    'p_value': float(p_val),
                    'normal': p_val > 0.05
                }
        
        # Homogeneity of variances
        levene_results = {}
        for factor in factors:
            groups = [group[dependent_var].values for name, group in data.groupby(factor)]
            stat, p_val = stats.levene(*groups)
            levene_results[factor] = {
                'statistic': float(stat),
                'p_value': float(p_val),
                'equal_variances': p_val > 0.05
            }
        
        # Post-hoc tests if significant
        posthoc_results = {}
        for idx, row in aov.iterrows():
            if row['p-unc'] < 0.05 and row['Source'] in factors:
                factor = row['Source']
                posthoc = pg.pairwise_ttests(data=data, dv=dependent_var, 
                                            between=factor, padjust='bonf')
                posthoc_results[factor] = posthoc.to_dict('records')
        
        # Effect sizes (eta-squared)
        aov['eta_squared'] = aov['SS'] / aov['SS'].sum()
        
        results = {
            'anova_table': aov.to_dict('records'),
            'assumptions': {
                'normality': normality_results,
                'homogeneity': levene_results
            },
            'post_hoc': posthoc_results,
            'interpretation': self._interpret_anova_results(aov)
        }
        
        return results
    
    def predict_learning_outcomes(self, data: pd.DataFrame, 
                                features: List[str], 
                                target: str) -> Dict:
        """Multiple regression analysis for predicting learning outcomes"""
        
        from sklearn.model_selection import train_test_split, cross_val_score
        from sklearn.linear_model import LinearRegression, Ridge
        from sklearn.preprocessing import StandardScaler, PolynomialFeatures
        from sklearn.metrics import r2_score, mean_squared_error
        
        # Prepare data
        X = data[features]
        y = data[target]
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Standardize features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Linear regression
        lr_model = LinearRegression()
        lr_model.fit(X_train_scaled, y_train)
        lr_pred = lr_model.predict(X_test_scaled)
        
        # Ridge regression
        ridge_model = Ridge(alpha=1.0)
        ridge_model.fit(X_train_scaled, y_train)
        ridge_pred = ridge_model.predict(X_test_scaled)
        
        # Polynomial features
        poly = PolynomialFeatures(degree=2, include_bias=False)
        X_train_poly = poly.fit_transform(X_train_scaled)
        X_test_poly = poly.transform(X_test_scaled)
        
        poly_model = LinearRegression()
        poly_model.fit(X_train_poly, y_train)
        poly_pred = poly_model.predict(X_test_poly)
        
        # Cross-validation
        cv_scores_lr = cross_val_score(lr_model, X_train_scaled, y_train, cv=5)
        cv_scores_ridge = cross_val_score(ridge_model, X_train_scaled, y_train, cv=5)
        
        # Feature importance (standardized coefficients)
        feature_importance = pd.DataFrame({
            'feature': features,
            'coefficient': lr_model.coef_,
            'abs_coefficient': np.abs(lr_model.coef_)
        }).sort_values('abs_coefficient', ascending=False)
        
        # Residual analysis
        residuals = y_test - lr_pred
        
        results = {
            'linear_regression': {
                'r2_score': float(r2_score(y_test, lr_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_test, lr_pred))),
                'cv_scores': cv_scores_lr.tolist(),
                'cv_mean': float(cv_scores_lr.mean()),
                'cv_std': float(cv_scores_lr.std())
            },
            'ridge_regression': {
                'r2_score': float(r2_score(y_test, ridge_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_test, ridge_pred))),
                'cv_scores': cv_scores_ridge.tolist(),
                'cv_mean': float(cv_scores_ridge.mean()),
                'cv_std': float(cv_scores_ridge.std())
            },
            'polynomial_regression': {
                'r2_score': float(r2_score(y_test, poly_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_test, poly_pred))),
                'n_features': int(X_train_poly.shape[1])
            },
            'feature_importance': feature_importance.to_dict('records'),
            'residual_analysis': {
                'mean_residual': float(residuals.mean()),
                'std_residual': float(residuals.std()),
                'shapiro_test': stats.shapiro(residuals)
            },
            'sample_predictions': pd.DataFrame({
                'actual': y_test.values[:10],
                'linear': lr_pred[:10],
                'ridge': ridge_pred[:10],
                'polynomial': poly_pred[:10]
            }).to_dict('records')
        }
        
        return results
    
    def _interpret_effect_size(self, d: float) -> str:
        """Interpret Cohen's d effect size"""
        if abs(d) < 0.2:
            return "Negligible effect"
        elif abs(d) < 0.5:
            return "Small effect"
        elif abs(d) < 0.8:
            return "Medium effect"
        else:
            return "Large effect"
    
    def _interpret_anova_results(self, aov_table: pd.DataFrame) -> Dict:
        """Interpret ANOVA results"""
        interpretations = {}
        
        for idx, row in aov_table.iterrows():
            source = row['Source']
            if source != 'Residual':
                sig = "significant" if row['p-unc'] < 0.05 else "not significant"
                effect = "small" if row['eta_squared'] < 0.06 else \
                        "medium" if row['eta_squared'] < 0.14 else "large"
                
                interpretations[source] = {
                    'significance': sig,
                    'p_value': float(row['p-unc']),
                    'effect_size': effect,
                    'eta_squared': float(row['eta_squared'])
                }
        
        return interpretations
